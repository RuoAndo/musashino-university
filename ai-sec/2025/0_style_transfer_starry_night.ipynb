{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] VGG19 features をロード\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "optim(L-BFGS):  67%|██████▋   | 200/300 [00:21<00:10,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] output.jpg\n",
      "[DONE] 出力: output.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "nst_vangogh_local_safe.py\n",
    "Neural Style Transfer (Gatys) — ローカル＆巨大画像安全版\n",
    "- 依存: torch torchvision pillow tqdm\n",
    "- 入力:\n",
    "    - style:   Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\n",
    "    - content: Tokyo-Tower.jpg\n",
    "- 出力: output.jpg\n",
    "- ポイント:\n",
    "    1) Pillow の DecompressionBomb を無効化 + 事前縮小で安全に読込\n",
    "    2) 進捗表示つき\n",
    "    3) LBFGS/Adam 切替可能\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "画風を強くしたい: STYLE_WEIGHT を上げる（例: 1e5 → 3e5）\n",
    "内容を残したい: CONTENT_WEIGHT を上げる\n",
    "解像度を上げたい: IMAGE_SIZE を上げる（GPUメモリと実行時間に注意）\n",
    "収束が荒い場合: NUM_STEPS を増やす、USE_LBFGS=True を推奨（高品質）\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "Image.MAX_IMAGE_PIXELS = None  # 巨大画像の安全上限を解除（必ず事前縮小も併用）\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# ====== 設定 ======\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = 1024 if torch.cuda.is_available() else 256   # 768/1024 などに上げると高精細（VRAMに注意）\n",
    "STYLE_PATH   = \"Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\"\n",
    "\n",
    "# CONTENT_PATH = \"yakei.png\"\n",
    "CONTENT_PATH = \"Central_Park_in_Shinjuku_Ward_Tokyo_20250824104812_01.png\"\n",
    "OUTPUT_PATH  = \"output.jpg\"\n",
    "\n",
    "CONTENT_WEIGHT = 1.0\n",
    "STYLE_WEIGHT   = 4e5\n",
    "NUM_STEPS      = 300             # 画質↑: 増やす（GPUなら 500-1000 も可）\n",
    "USE_LBFGS      = True            # False で Adam に切替（高速デバッグ向け）\n",
    "LR_ADAM        = 1e-1\n",
    "\n",
    "# ====== ユーティリティ ======\n",
    "def require_file(path, label):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"[{label}] {path} が見つかりません。同じフォルダに置いてください。\")\n",
    "\n",
    "def safe_load_and_resize(path, imsize=IMAGE_SIZE):\n",
    "    \"\"\"\n",
    "    DecompressionBombError を回避しつつ、事前に強制縮小してからテンソル化。\n",
    "    ・EXIFの回転を補正\n",
    "    ・thumbnail で長辺 imsize へクリップ\n",
    "    \"\"\"\n",
    "    with Image.open(path) as im:\n",
    "        im = ImageOps.exif_transpose(im).convert(\"RGB\")\n",
    "        im.thumbnail((imsize, imsize), Image.LANCZOS)  # ここで強制縮小\n",
    "        to_tensor = transforms.ToTensor()\n",
    "        t = to_tensor(im).unsqueeze(0).to(DEVICE, torch.float)\n",
    "    return t\n",
    "\n",
    "# VGG 前処理の平均・分散\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(DEVICE)\n",
    "cnn_normalization_std  = torch.tensor([0.229, 0.224, 0.225]).to(DEVICE)\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = mean.clone().detach().view(-1,1,1)\n",
    "        self.std  = std.clone().detach().view(-1,1,1)\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "def unnormalize_and_save(tensor, path):\n",
    "    x = tensor.detach().cpu().squeeze(0)\n",
    "    x = x * cnn_normalization_std.view(3,1,1) + cnn_normalization_mean.view(3,1,1)\n",
    "    x = torch.clamp(x, 0, 1)\n",
    "    transforms.ToPILImage()(x).save(path)\n",
    "    print(f\"[SAVE] {path}\")\n",
    "\n",
    "def gram_matrix(x):\n",
    "    b, c, h, w = x.size()\n",
    "    F = x.view(b, c, h*w)\n",
    "    return torch.bmm(F, F.transpose(1,2)) / (c*h*w)\n",
    "\n",
    "# 使う層\n",
    "content_layers_default = ['conv_4']\n",
    "style_layers_default   = ['conv_1','conv_2','conv_3','conv_4','conv_5']\n",
    "\n",
    "class StyleTransferModel(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG を順に通し、指定層の出力を収集。\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn, mean, std,\n",
    "                 content_layers=content_layers_default,\n",
    "                 style_layers=style_layers_default):\n",
    "        super().__init__()\n",
    "        self.content_layers = content_layers\n",
    "        self.style_layers   = style_layers\n",
    "        self.model = nn.Sequential(Normalization(mean, std))\n",
    "\n",
    "        i = 0\n",
    "        for layer in cnn.children():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                i += 1; name = f'conv_{i}'\n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                name = f'relu_{i}'; layer = nn.ReLU(inplace=False)\n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                name = f'pool_{i}'\n",
    "            elif isinstance(layer, nn.BatchNorm2d):\n",
    "                name = f'bn_{i}'\n",
    "            else:\n",
    "                name = f'layer_{i}'\n",
    "            self.model.add_module(name, layer)\n",
    "            # conv_5 近辺までで十分\n",
    "            if i >= 5 and isinstance(layer, nn.ReLU):\n",
    "                break\n",
    "\n",
    "    def forward(self, x):\n",
    "        c_feats, s_feats = {}, {}\n",
    "        for name, layer in self.model._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.content_layers: c_feats[name] = x\n",
    "            if name in self.style_layers:   s_feats[name] = x\n",
    "        return c_feats, s_feats\n",
    "\n",
    "def run_style_transfer(cnn, mean, std, content_img, style_img, input_img,\n",
    "                       steps=NUM_STEPS, cw=CONTENT_WEIGHT, sw=STYLE_WEIGHT):\n",
    "    model = StyleTransferModel(cnn, mean, std).to(DEVICE).eval()\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        c_ref, _ = model(content_img)\n",
    "        _, s_ref = model(style_img)\n",
    "        s_grams = {l: gram_matrix(s_ref[l]) for l in s_ref}\n",
    "\n",
    "    x = input_img.clone().requires_grad_(True)\n",
    "\n",
    "    if USE_LBFGS:\n",
    "        opt = optim.LBFGS([x])\n",
    "        run = [0]\n",
    "        pbar = tqdm(total=steps, desc=\"optim(L-BFGS)\")\n",
    "        def closure():\n",
    "            opt.zero_grad()\n",
    "            c_out, s_out = model(x)\n",
    "            c_loss = sum(mse(c_out[l], c_ref[l]) for l in c_out)\n",
    "            s_loss = sum(mse(gram_matrix(s_out[l]), s_grams[l]) for l in s_out)\n",
    "            loss = cw*c_loss + sw*s_loss\n",
    "            loss.backward()\n",
    "            run[0] += 1\n",
    "            # ざっくり進捗（LBFGSは内部で複数回呼ばれるため10刻みで更新）\n",
    "            if pbar.n < steps:\n",
    "                pbar.update(min(10, steps - pbar.n))\n",
    "            return loss\n",
    "        opt.step(closure)\n",
    "        pbar.close()\n",
    "    else:\n",
    "        opt = optim.Adam([x], lr=LR_ADAM)\n",
    "        pbar = tqdm(range(steps), desc=\"optim(Adam)\")\n",
    "        for _ in pbar:\n",
    "            opt.zero_grad()\n",
    "            c_out, s_out = model(x)\n",
    "            c_loss = sum(mse(c_out[l], c_ref[l]) for l in c_out)\n",
    "            s_loss = sum(mse(gram_matrix(s_out[l]), s_grams[l]) for l in s_out)\n",
    "            loss = cw*c_loss + sw*s_loss\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    return x.detach()\n",
    "\n",
    "def main():\n",
    "    require_file(STYLE_PATH,   \"style\")\n",
    "    require_file(CONTENT_PATH, \"content\")\n",
    "\n",
    "    content_img = safe_load_and_resize(CONTENT_PATH, IMAGE_SIZE)\n",
    "    style_img   = safe_load_and_resize(STYLE_PATH,   IMAGE_SIZE)\n",
    "\n",
    "    print(\"[INFO] VGG19 features をロード\")\n",
    "    vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.to(DEVICE).eval()\n",
    "\n",
    "    input_img = content_img.clone()\n",
    "    output = run_style_transfer(vgg, cnn_normalization_mean, cnn_normalization_std,\n",
    "                                content_img, style_img, input_img,\n",
    "                                steps=NUM_STEPS, cw=CONTENT_WEIGHT, sw=STYLE_WEIGHT)\n",
    "\n",
    "    unnormalize_and_save(output, OUTPUT_PATH)\n",
    "    print(\"[DONE] 出力:\", OUTPUT_PATH)\n",
    "\n",
    "    Image.open(OUTPUT_PATH).show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
