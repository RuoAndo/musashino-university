{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3dd1315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A] Build txId index from features (txId column only) ...\n",
      "[A] tx nodes: 203769  saved: ./_prep\\tx_ids.npy\n",
      "[B] Build features memmap ...\n",
      "[B] feature dim D=182 (saved columns: ./_prep\\feature_cols.npy)\n",
      "[B] features memmap saved: ./_prep\\x_float32.memmap\n",
      "[C] Build labels y ...\n",
      "[C] labels saved: ./_prep\\y_int8.npy  (updated 203769 rows)\n",
      "[D] Build edge_index ...\n",
      "[D] edges kept: 234355  saved: ./_prep\\edge_index.npy\n",
      "[INFO] device=cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_99988\\2340660716.py:251: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:209.)\n",
      "  x = torch.from_numpy(x_mm)  # 参照のみ（巨大でもRAMに全コピーしにくい）\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] start\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "'NeighborSampler' requires either 'pyg-lib' or 'torch-sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 372\u001b[0m\n\u001b[0;32m    368\u001b[0m     print_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DONE]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 372\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 310\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    308\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    309\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 310\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\loader\\node_loader.py:147\u001b[0m, in \u001b[0;36mNodeLoader.collate_fn\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Samples a subgraph from a batch of input nodes.\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m input_data: NodeSamplerInput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_data[index]\n\u001b[1;32m--> 147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_per_worker:  \u001b[38;5;66;03m# Execute `filter_fn` in the worker process\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_fn(out)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\sampler\\neighbor_sampler.py:403\u001b[0m, in \u001b[0;36mNeighborSampler.sample_from_nodes\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msample_from_nodes\u001b[39m(\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    401\u001b[0m     inputs: NodeSamplerInput,\n\u001b[0;32m    402\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[SamplerOutput, HeteroSamplerOutput]:\n\u001b[1;32m--> 403\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnode_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubgraph_type \u001b[38;5;241m==\u001b[39m SubgraphType\u001b[38;5;241m.\u001b[39mbidirectional:\n\u001b[0;32m    405\u001b[0m         out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto_bidirectional(keep_orig_edges\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_orig_edges)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\sampler\\neighbor_sampler.py:815\u001b[0m, in \u001b[0;36mnode_sample\u001b[1;34m(inputs, sample_fn)\u001b[0m\n\u001b[0;32m    812\u001b[0m     seed \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mnode\n\u001b[0;32m    813\u001b[0m     seed_time \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mtime\n\u001b[1;32m--> 815\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43msample_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    816\u001b[0m out\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m (inputs\u001b[38;5;241m.\u001b[39minput_id, inputs\u001b[38;5;241m.\u001b[39mtime)\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\sampler\\neighbor_sampler.py:606\u001b[0m, in \u001b[0;36mNeighborSampler._sample\u001b[1;34m(self, seed, seed_time, **kwargs)\u001b[0m\n\u001b[0;32m    603\u001b[0m     num_sampled_nodes \u001b[38;5;241m=\u001b[39m num_sampled_edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    607\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meither \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyg-lib\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch-sparse\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_direction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    610\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m col, row\n",
      "\u001b[1;31mImportError\u001b[0m: 'NeighborSampler' requires either 'pyg-lib' or 'torch-sparse'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 設定\n",
    "# =========================\n",
    "TXS_FEATURES = \"txs_features.txt\"\n",
    "TXS_CLASSES  = \"txs_classes.txt\"\n",
    "TXS_EDGES    = \"txs_edgelist.txt\"\n",
    "\n",
    "WORKDIR = \"./_prep\"  # 生成物（memmapなど）置き場\n",
    "EMB_DIM = 64         # ← ここが「64次元」の指定\n",
    "HID_DIM = 128\n",
    "EPOCHS = 3           # とりあえず短め。増やすと精度は上がる\n",
    "BATCH_SIZE = 4096\n",
    "NUM_NEIGHBORS = [15, 10]  # 2層分の近傍サンプル数\n",
    "LR = 1e-3\n",
    "SEED = 42\n",
    "\n",
    "# class の解釈（あなたのファイルを見ると 3 が出ている）\n",
    "# 典型: 1=illicit, 2=licit, 3=unknown\n",
    "CLASS_TO_Y = {1: 1, 2: 0, 3: -1}  # y: 1=illicit, 0=licit, -1=unknown(学習除外)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ユーティリティ\n",
    "# =========================\n",
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def fast_line_count(path: str) -> int:\n",
    "    # ヘッダ含めた行数：巨大でもそこそこ速い\n",
    "    # Windowsならこれが最速級だが、ここではPython内でカウント\n",
    "    n = 0\n",
    "    with open(path, \"rb\") as f:\n",
    "        for _ in f:\n",
    "            n += 1\n",
    "    return n\n",
    "\n",
    "def ensure_dir(d: str):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def infer_feature_cols(header_cols):\n",
    "    # txId, Time step 以外を全部 feature として使う\n",
    "    drop = {\"txId\", \"Time step\"}\n",
    "    feat_cols = [c for c in header_cols if c not in drop]\n",
    "    return feat_cols\n",
    "\n",
    "def print_once(msg: str):\n",
    "    print(msg, flush=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ステップA: txId -> row index のマップを作る（featuresの順番を基準）\n",
    "# =========================\n",
    "def build_txid_index(features_path: str, out_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    features の txId 列だけを読み、txId の配列を保存して index を確定する。\n",
    "    \"\"\"\n",
    "    print_once(\"[A] Build txId index from features (txId column only) ...\")\n",
    "    # txId列だけ読む（メモリ節約）\n",
    "    tx = pd.read_csv(features_path, usecols=[\"txId\"], dtype={\"txId\": np.int64})\n",
    "    tx_ids = tx[\"txId\"].to_numpy(np.int64)\n",
    "    np.save(out_path, tx_ids)\n",
    "    print_once(f\"[A] tx nodes: {len(tx_ids)}  saved: {out_path}\")\n",
    "    return tx_ids\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ステップB: 特徴量を memmap (float32) に落とす\n",
    "# =========================\n",
    "def build_features_memmap(features_path: str, tx_ids: np.ndarray, out_memmap_path: str, out_cols_path: str):\n",
    "    \"\"\"\n",
    "    features を chunk で読み、float32 の memmap に書き込む。\n",
    "    形状: [N, D] （D=183 になるはず）\n",
    "    \"\"\"\n",
    "    print_once(\"[B] Build features memmap ...\")\n",
    "    header = pd.read_csv(features_path, nrows=0)\n",
    "    feat_cols = infer_feature_cols(list(header.columns))\n",
    "    D = len(feat_cols)\n",
    "    np.save(out_cols_path, np.array(feat_cols, dtype=object))\n",
    "    print_once(f\"[B] feature dim D={D} (saved columns: {out_cols_path})\")\n",
    "\n",
    "    N = len(tx_ids)\n",
    "    mm = np.memmap(out_memmap_path, dtype=np.float32, mode=\"w+\", shape=(N, D))\n",
    "\n",
    "    # chunk読み（RAM節約）\n",
    "    chunksize = 200_000\n",
    "    reader = pd.read_csv(\n",
    "        features_path,\n",
    "        usecols=feat_cols,\n",
    "        dtype={c: np.float32 for c in feat_cols},\n",
    "        chunksize=chunksize,\n",
    "    )\n",
    "\n",
    "    offset = 0\n",
    "    for chunk in reader:\n",
    "        arr = chunk.to_numpy(np.float32, copy=False)\n",
    "        n = arr.shape[0]\n",
    "        mm[offset:offset+n, :] = arr\n",
    "        offset += n\n",
    "        if offset % 1_000_000 < chunksize:\n",
    "            print_once(f\"[B] written rows: {offset}/{N}\")\n",
    "\n",
    "    mm.flush()\n",
    "    if offset != N:\n",
    "        raise RuntimeError(f\"features rows mismatch: wrote={offset} N={N}\")\n",
    "    print_once(f\"[B] features memmap saved: {out_memmap_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ステップC: ラベル配列 y を作る（unknown=-1）\n",
    "# =========================\n",
    "def build_labels(classes_path: str, tx_ids: np.ndarray, out_path: str) -> np.ndarray:\n",
    "    print_once(\"[C] Build labels y ...\")\n",
    "    # txId -> index を作る（dictは重いが txId数が数百万なら許容範囲のことが多い）\n",
    "    # メモリが厳しければ、別方式（ソート＆サーチ）に置き換え可能\n",
    "    id_to_idx = {int(t): i for i, t in enumerate(tx_ids)}\n",
    "\n",
    "    y = np.full(shape=(len(tx_ids),), fill_value=-1, dtype=np.int8)  # unknown=-1\n",
    "    df = pd.read_csv(classes_path, dtype={\"txId\": np.int64, \"class\": np.int16})\n",
    "\n",
    "    updated = 0\n",
    "    for txId, cls in zip(df[\"txId\"].to_numpy(), df[\"class\"].to_numpy()):\n",
    "        idx = id_to_idx.get(int(txId), None)\n",
    "        if idx is None:\n",
    "            continue\n",
    "        y_val = CLASS_TO_Y.get(int(cls), -1)\n",
    "        y[idx] = y_val\n",
    "        updated += 1\n",
    "\n",
    "    np.save(out_path, y)\n",
    "    print_once(f\"[C] labels saved: {out_path}  (updated {updated} rows)\")\n",
    "    return y\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ステップD: edge_index を作る（txId -> index に変換）\n",
    "# =========================\n",
    "def build_edge_index(edges_path: str, tx_ids: np.ndarray, out_path: str) -> np.ndarray:\n",
    "    print_once(\"[D] Build edge_index ...\")\n",
    "    id_to_idx = {int(t): i for i, t in enumerate(tx_ids)}\n",
    "\n",
    "    # edgesは比較的小さい（あなたのサイズだと数十万程度）ので一括読みOK\n",
    "    edges = pd.read_csv(edges_path, dtype={\"txId1\": np.int64, \"txId2\": np.int64})\n",
    "    src = edges[\"txId1\"].to_numpy(np.int64)\n",
    "    dst = edges[\"txId2\"].to_numpy(np.int64)\n",
    "\n",
    "    src_idx = np.empty_like(src)\n",
    "    dst_idx = np.empty_like(dst)\n",
    "\n",
    "    valid = np.ones(len(src), dtype=bool)\n",
    "    for i in range(len(src)):\n",
    "        a = id_to_idx.get(int(src[i]), None)\n",
    "        b = id_to_idx.get(int(dst[i]), None)\n",
    "        if a is None or b is None:\n",
    "            valid[i] = False\n",
    "            continue\n",
    "        src_idx[i] = a\n",
    "        dst_idx[i] = b\n",
    "\n",
    "    src_idx = src_idx[valid]\n",
    "    dst_idx = dst_idx[valid]\n",
    "\n",
    "    edge_index = np.vstack([src_idx, dst_idx]).astype(np.int64)\n",
    "    np.save(out_path, edge_index)\n",
    "    print_once(f\"[D] edges kept: {edge_index.shape[1]}  saved: {out_path}\")\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "# =========================\n",
    "# GraphSAGE モデル\n",
    "# =========================\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int, out_dim: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden)\n",
    "        self.conv2 = SAGEConv(hidden, out_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "# =========================\n",
    "# メイン：準備→学習→埋め込み保存\n",
    "# =========================\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "    ensure_dir(WORKDIR)\n",
    "\n",
    "    # 生成物パス\n",
    "    txids_path = os.path.join(WORKDIR, \"tx_ids.npy\")\n",
    "    feat_mm_path = os.path.join(WORKDIR, \"x_float32.memmap\")\n",
    "    feat_cols_path = os.path.join(WORKDIR, \"feature_cols.npy\")\n",
    "    y_path = os.path.join(WORKDIR, \"y_int8.npy\")\n",
    "    edge_path = os.path.join(WORKDIR, \"edge_index.npy\")\n",
    "    emb_out_path = os.path.join(WORKDIR, f\"emb_{EMB_DIM}d_float32.memmap\")\n",
    "\n",
    "    # A: txId index\n",
    "    if os.path.exists(txids_path):\n",
    "        tx_ids = np.load(txids_path)\n",
    "        print_once(f\"[A] reuse: {txids_path}  N={len(tx_ids)}\")\n",
    "    else:\n",
    "        tx_ids = build_txid_index(TXS_FEATURES, txids_path)\n",
    "\n",
    "    # B: features memmap\n",
    "    if os.path.exists(feat_mm_path) and os.path.exists(feat_cols_path):\n",
    "        feat_cols = np.load(feat_cols_path, allow_pickle=True)\n",
    "        D = len(feat_cols)\n",
    "        print_once(f\"[B] reuse memmap: {feat_mm_path}  shape=({len(tx_ids)}, {D})\")\n",
    "    else:\n",
    "        build_features_memmap(TXS_FEATURES, tx_ids, feat_mm_path, feat_cols_path)\n",
    "        feat_cols = np.load(feat_cols_path, allow_pickle=True)\n",
    "        D = len(feat_cols)\n",
    "\n",
    "    # C: labels\n",
    "    if os.path.exists(y_path):\n",
    "        y = np.load(y_path)\n",
    "        print_once(f\"[C] reuse: {y_path}\")\n",
    "    else:\n",
    "        y = build_labels(TXS_CLASSES, tx_ids, y_path)\n",
    "\n",
    "    # D: edges\n",
    "    if os.path.exists(edge_path):\n",
    "        edge_index = np.load(edge_path)\n",
    "        print_once(f\"[D] reuse: {edge_path}  E={edge_index.shape[1]}\")\n",
    "    else:\n",
    "        edge_index = build_edge_index(TXS_EDGES, tx_ids, edge_path)\n",
    "\n",
    "    # --- torch tensor にする（features は memmap を Tensor 化） ---\n",
    "    # memmap → torch (cpu) 参照\n",
    "    x_mm = np.memmap(feat_mm_path, dtype=np.float32, mode=\"r\", shape=(len(tx_ids), D))\n",
    "    x = torch.from_numpy(x_mm)  # 参照のみ（巨大でもRAMに全コピーしにくい）\n",
    "    y_t = torch.from_numpy(y.astype(np.int64))\n",
    "    edge_t = torch.from_numpy(edge_index)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_t, y=y_t)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print_once(f\"[INFO] device={device}\")\n",
    "\n",
    "    # --- mask（unknown=-1 を学習除外） ---\n",
    "    labeled = (data.y != -1)\n",
    "    idx = labeled.nonzero(as_tuple=False).view(-1)\n",
    "\n",
    "    # train/val を簡易分割\n",
    "    perm = idx[torch.randperm(idx.numel())]\n",
    "    n_train = int(0.8 * perm.numel())\n",
    "    train_idx = perm[:n_train]\n",
    "    val_idx = perm[n_train:]\n",
    "\n",
    "    # NeighborLoader（ミニバッチ学習）\n",
    "    train_loader = NeighborLoader(\n",
    "        data,\n",
    "        input_nodes=train_idx,\n",
    "        num_neighbors=NUM_NEIGHBORS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = NeighborLoader(\n",
    "        data,\n",
    "        input_nodes=val_idx,\n",
    "        num_neighbors=NUM_NEIGHBORS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    model = GraphSAGE(in_dim=D, hidden=HID_DIM, out_dim=EMB_DIM, dropout=0.2).to(device)\n",
    "    clf = torch.nn.Linear(EMB_DIM, 2).to(device)  # 2値（licit=0, illicit=1）\n",
    "\n",
    "    opt = torch.optim.Adam(list(model.parameters()) + list(clf.parameters()), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "    def batch_loss(batch):\n",
    "        batch = batch.to(device)\n",
    "        z = model(batch.x, batch.edge_index)  # [batch_size, EMB_DIM] ではなくサブグラフ全ノード分が出る\n",
    "        # input_nodes が先頭に来る仕様：batch.batch_size が対象ノード数\n",
    "        z0 = z[:batch.batch_size]\n",
    "        y0 = batch.y[:batch.batch_size]\n",
    "        # unknownは入ってこない想定だが念のため除外\n",
    "        m = (y0 != -1)\n",
    "        z0 = z0[m]\n",
    "        y0 = y0[m]\n",
    "        logits = clf(z0)\n",
    "        return F.cross_entropy(logits, y0)\n",
    "\n",
    "    # --- 学習 ---\n",
    "    print_once(\"[TRAIN] start\")\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train(); clf.train()\n",
    "        total = 0.0\n",
    "        steps = 0\n",
    "        for batch in train_loader:\n",
    "            opt.zero_grad()\n",
    "            loss = batch_loss(batch)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += float(loss.item())\n",
    "            steps += 1\n",
    "        avg = total / max(1, steps)\n",
    "\n",
    "        # 簡易val\n",
    "        model.eval(); clf.eval()\n",
    "        correct = 0; n = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                z = model(batch.x, batch.edge_index)\n",
    "                z0 = z[:batch.batch_size]\n",
    "                y0 = batch.y[:batch.batch_size]\n",
    "                m = (y0 != -1)\n",
    "                z0 = z0[m]; y0 = y0[m]\n",
    "                pred = clf(z0).argmax(dim=-1)\n",
    "                correct += int((pred == y0).sum().item())\n",
    "                n += int(y0.numel())\n",
    "        acc = correct / max(1, n)\n",
    "        print_once(f\"[TRAIN] epoch={epoch} loss={avg:.4f} val_acc={acc:.4f}\")\n",
    "\n",
    "    # --- 全ノードの 64次元埋め込み生成（バッチで回す） ---\n",
    "    # ここが「トランザクションごとに64次元」になる部分\n",
    "    print_once(\"[EMB] generate embeddings for ALL nodes ...\")\n",
    "\n",
    "    all_idx = torch.arange(data.num_nodes)\n",
    "    full_loader = NeighborLoader(\n",
    "        data,\n",
    "        input_nodes=all_idx,\n",
    "        num_neighbors=NUM_NEIGHBORS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    emb_mm = np.memmap(emb_out_path, dtype=np.float32, mode=\"w+\", shape=(data.num_nodes, EMB_DIM))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        offset = 0\n",
    "        for batch in full_loader:\n",
    "            batch = batch.to(device)\n",
    "            z = model(batch.x, batch.edge_index)\n",
    "            z0 = z[:batch.batch_size].detach().cpu().numpy().astype(np.float32, copy=False)\n",
    "            bsz = z0.shape[0]\n",
    "            # batch.n_id[:batch.batch_size] が元グラフのノードID（index）\n",
    "            nid = batch.n_id[:batch.batch_size].cpu().numpy()\n",
    "            emb_mm[nid, :] = z0\n",
    "            offset += bsz\n",
    "            if offset % 1_000_000 < BATCH_SIZE:\n",
    "                print_once(f\"[EMB] done nodes: {offset}/{data.num_nodes}\")\n",
    "\n",
    "    emb_mm.flush()\n",
    "    print_once(f\"[EMB] saved memmap: {emb_out_path}\")\n",
    "    print_once(\"[DONE]\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
