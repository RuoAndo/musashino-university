{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, zipfile, re, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# ===== Settings =====\n",
    "RATE = 0.30  # Outlier rate 30%\n",
    "\n",
    "# Console UTF-8 (mojibake guard for print)\n",
    "try:\n",
    "    sys.stdout.reconfigure(encoding=\"utf-8\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Matplotlib minus sign fix (mojibake guard)\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "ZIP_PATH    = \"./isolation_forest.zip\"   # adjust if needed\n",
    "EXTRACT_DIR = \"./isolation_forest_all\"\n",
    "\n",
    "# ===== Unzip =====\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "    z.extractall(EXTRACT_DIR)\n",
    "\n",
    "# ===== Collect CSVs =====\n",
    "csv_files = []\n",
    "for root, _, files in os.walk(EXTRACT_DIR):\n",
    "    for f in files:\n",
    "        if f.lower().endswith(\".csv\"):\n",
    "            csv_files.append(os.path.join(root, f))\n",
    "csv_files.sort()\n",
    "print(f\"CSV files found: {len(csv_files)}\")\n",
    "\n",
    "# ===== Helpers =====\n",
    "IPV4_RE = re.compile(r\"\\b(?:(?:25[0-5]|2[0-4]\\d|1?\\d?\\d)\\.){3}(?:25[0-5]|2[0-4]\\d|1?\\d?\\d)\\b\")\n",
    "\n",
    "def detect_lat_lon_columns(columns):\n",
    "    # exact first\n",
    "    lat = next((c for c in columns if c.lower() in [\"latitude\", \"lat\", \"y\", \"lat_deg\"]), None)\n",
    "    lon = next((c for c in columns if c.lower() in [\"longitude\", \"lon\", \"lng\", \"x\", \"long\", \"lon_deg\"]), None)\n",
    "    # fallback partial\n",
    "    if lat is None:\n",
    "        lat = next((c for c in columns if \"lat\" in c.lower()), None)\n",
    "    if lon is None:\n",
    "        lon = next((c for c in columns if (\"lon\" in c.lower()) or (\"lng\" in c.lower()) or (\"longi\" in c.lower())), None)\n",
    "    return lat, lon\n",
    "\n",
    "def detect_ip_column(df: pd.DataFrame):\n",
    "    cols  = list(df.columns)\n",
    "    lower = [c.lower() for c in cols]\n",
    "    prefs = [\"ip\", \"ip_address\", \"source_ip\", \"src_ip\", \"dst_ip\", \"ipaddr\", \"address\", \"remote_ip\", \"client_ip\"]\n",
    "    for p in prefs:\n",
    "        if p in lower:\n",
    "            return cols[lower.index(p)]\n",
    "    for c in cols:\n",
    "        if \"ip\" in c.lower():\n",
    "            return c\n",
    "    # text columns: pick one with >=10% ipv4 presence\n",
    "    text_cols = [c for c in cols if df[c].dtype == \"object\"]\n",
    "    best_col, best_rate = None, 0.0\n",
    "    sample_n = min(200, len(df))\n",
    "    for c in text_cols:\n",
    "        rate = df[c].astype(str).head(sample_n).apply(lambda s: IPV4_RE.search(s) is not None).mean()\n",
    "        if rate > best_rate:\n",
    "            best_rate, best_col = rate, c\n",
    "    if best_col is not None and best_rate >= 0.10:\n",
    "        return best_col\n",
    "    return None\n",
    "\n",
    "def filename_to_datetime(fname: str):\n",
    "    m14 = re.search(r\"(\\d{14})\", fname)\n",
    "    if m14:\n",
    "        return pd.to_datetime(m14.group(1), format=\"%Y%m%d%H%M%S\", errors=\"coerce\")\n",
    "    m8 = re.search(r\"(\\d{8})\", fname)\n",
    "    if m8:\n",
    "        return pd.to_datetime(m8.group(1), format=\"%Y%m%d\", errors=\"coerce\")\n",
    "    return pd.NaT\n",
    "\n",
    "def extract_outlier_ip_set(df: pd.DataFrame, is_out_mask: np.ndarray) -> set:\n",
    "    \"\"\"Return unique IPv4 set from outlier rows; falls back to scanning text columns.\"\"\"\n",
    "    ip_col = detect_ip_column(df)\n",
    "    ips = set()\n",
    "    if ip_col:\n",
    "        for s in df.loc[is_out_mask, ip_col].astype(str):\n",
    "            m = IPV4_RE.search(s)\n",
    "            if m:\n",
    "                ips.add(m.group(0))\n",
    "        return ips\n",
    "    # fallback: scan concatenated text columns\n",
    "    text_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    if text_cols:\n",
    "        concat_text = df[text_cols].astype(str).agg(\" \".join, axis=1)\n",
    "        for s in concat_text[is_out_mask]:\n",
    "            m = IPV4_RE.search(s)\n",
    "            if m:\n",
    "                ips.add(m.group(0))\n",
    "    return ips\n",
    "\n",
    "# ===== Main (LOF with RATE) =====\n",
    "per_file = []\n",
    "\n",
    "for path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] read error {path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    lat_col, lon_col = detect_lat_lon_columns(df.columns)\n",
    "    if not lat_col or not lon_col:\n",
    "        print(f\"[SKIP] {os.path.basename(path)}: lat/lon not found -> {list(df.columns)}\")\n",
    "        continue\n",
    "\n",
    "    # Clean\n",
    "    df = df.copy()\n",
    "    df[lat_col] = pd.to_numeric(df[lat_col], errors=\"coerce\")\n",
    "    df[lon_col] = pd.to_numeric(df[lon_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[lat_col, lon_col])\n",
    "    df = df[(df[lat_col].between(-90, 90)) & (df[lon_col].between(-180, 180))]\n",
    "    if len(df) < 5:\n",
    "        print(f\"[SKIP] {os.path.basename(path)}: too few rows (n={len(df)})\")\n",
    "        continue\n",
    "\n",
    "    X = df[[lon_col, lat_col]].to_numpy()\n",
    "    n = len(X)\n",
    "\n",
    "    # LOF (30%): unify contamination and neighbor scale\n",
    "    n_neighbors = max(10, min(75, int(n * RATE)))\n",
    "    if n_neighbors >= n:\n",
    "        n_neighbors = max(5, n - 1)\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=RATE)\n",
    "    y_pred = lof.fit_predict(X)   # 1=inlier, -1=outlier\n",
    "    is_out = (y_pred == -1)\n",
    "\n",
    "    ip_set = extract_outlier_ip_set(df, is_out)\n",
    "    outlier_ip_count    = len(ip_set)       # unique outlier IPs\n",
    "    outlier_point_count = int(is_out.sum()) # outlier coordinates (reference)\n",
    "\n",
    "    fname = os.path.basename(path).replace(\".csv\", \"\")\n",
    "    dt    = filename_to_datetime(fname)\n",
    "    date  = pd.to_datetime(dt.date()) if pd.notna(dt) else pd.NaT\n",
    "\n",
    "    per_file.append({\n",
    "        \"file\": fname,\n",
    "        \"datetime\": dt,\n",
    "        \"date\": date,\n",
    "        \"outlier_ip_count\": outlier_ip_count,\n",
    "        \"outlier_point_count\": outlier_point_count,\n",
    "        \"ip_set\": ip_set,\n",
    "        \"n_neighbors\": n_neighbors,\n",
    "        \"n_rows\": n\n",
    "    })\n",
    "\n",
    "per_file_df = pd.DataFrame(per_file).dropna(subset=[\"datetime\"]).sort_values(\"datetime\")\n",
    "\n",
    "if per_file_df.empty:\n",
    "    print(\"No valid data to analyze (lat/lon missing or date parse failed).\")\n",
    "else:\n",
    "    # ---- Plot per-file (datetime) unique outlier IP count ----\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(per_file_df[\"datetime\"], per_file_df[\"outlier_ip_count\"], marker=\"o\")\n",
    "    plt.title(f\"Per-file unique outlier IPs (LOF {int(RATE*100)}%)\")\n",
    "    plt.xlabel(\"Datetime\")\n",
    "    plt.ylabel(\"Unique outlier IPs\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # (reference) outlier points count\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(per_file_df[\"datetime\"], per_file_df[\"outlier_point_count\"], marker=\"o\")\n",
    "    plt.title(f\"Per-file outlier points (LOF {int(RATE*100)}%)\")\n",
    "    plt.xlabel(\"Datetime\")\n",
    "    plt.ylabel(\"Outlier points\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---- Daily union (unique IPs across files on same day) ----\n",
    "    day_union = {}\n",
    "    for _, row in per_file_df.dropna(subset=[\"date\"]).iterrows():\n",
    "        d = row[\"date\"]\n",
    "        s = row[\"ip_set\"] if isinstance(row[\"ip_set\"], set) else set()\n",
    "        if d not in day_union:\n",
    "            day_union[d] = set()\n",
    "        day_union[d] |= s\n",
    "\n",
    "    per_day = [{\"date\": d, \"unique_outlier_ips_union\": len(day_union[d])} for d in sorted(day_union.keys())]\n",
    "    per_day_df = pd.DataFrame(per_day)\n",
    "\n",
    "    if not per_day_df.empty:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(per_day_df[\"date\"], per_day_df[\"unique_outlier_ips_union\"], marker=\"o\")\n",
    "        # ASCII-only title to avoid mojibake\n",
    "        plt.title(f\"Daily unique outlier IPs (union) - LOF {int(RATE*100)}%\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Unique outlier IPs\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # ---- Print heads for sanity check ----\n",
    "    print(\"\\n--- Per-file summary (head) ---\")\n",
    "    print(per_file_df[[\"file\",\"datetime\",\"outlier_ip_count\",\"outlier_point_count\",\"n_neighbors\",\"n_rows\"]].head(10).to_string(index=False))\n",
    "\n",
    "    if not per_day_df.empty:\n",
    "        print(\"\\n--- Daily union (unique outlier IPs) ---\")\n",
    "        print(per_day_df.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
