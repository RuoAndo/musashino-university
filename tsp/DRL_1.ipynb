{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc2abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   200 | train_mean 4.3001 | greedy_mean 4.2173 | baseline 4.2768 | loss -0.2785 | 196.9s\n",
      "step   400 | train_mean 4.1051 | greedy_mean 4.1409 | baseline 4.1519 | loss 0.0507 | 402.7s\n",
      "step   600 | train_mean 4.1287 | greedy_mean 4.1116 | baseline 4.1322 | loss -0.0148 | 611.9s\n",
      "step   800 | train_mean 4.1563 | greedy_mean 4.1247 | baseline 4.1404 | loss -0.0690 | 819.5s\n",
      "step  1000 | train_mean 4.1445 | greedy_mean 4.1074 | baseline 4.1153 | loss -0.1216 | 1025.2s\n",
      "step  1200 | train_mean 4.1106 | greedy_mean 4.1193 | baseline 4.1252 | loss -0.0496 | 1250.0s\n",
      "step  1400 | train_mean 4.1043 | greedy_mean 4.0890 | baseline 4.1048 | loss -0.0492 | 1910.8s\n",
      "step  1600 | train_mean 4.1096 | greedy_mean 4.1175 | baseline 4.1126 | loss -0.0707 | 2780.8s\n",
      "step  1800 | train_mean 4.0764 | greedy_mean 4.0662 | baseline 4.0955 | loss -0.0414 | 3434.5s\n",
      "step  2000 | train_mean 4.0439 | greedy_mean 4.0696 | baseline 4.0794 | loss -0.0143 | 3643.7s\n",
      "step  2200 | train_mean 4.0871 | greedy_mean 4.0729 | baseline 4.0536 | loss -0.0827 | 3854.2s\n",
      "step  2400 | train_mean 4.1036 | greedy_mean 4.0686 | baseline 4.0754 | loss -0.1046 | 4063.9s\n",
      "step  2600 | train_mean 4.0868 | greedy_mean 4.0799 | baseline 4.0616 | loss -0.0483 | 4304.4s\n",
      "step  2800 | train_mean 4.0576 | greedy_mean 4.0479 | baseline 4.0654 | loss -0.0525 | 4547.8s\n",
      "step  3000 | train_mean 4.0745 | greedy_mean 4.0643 | baseline 4.0856 | loss -0.0363 | 4790.7s\n",
      "step  3200 | train_mean 4.0723 | greedy_mean 4.0410 | baseline 4.0516 | loss -0.0164 | 5035.9s\n",
      "step  3400 | train_mean 4.0153 | greedy_mean 4.0514 | baseline 4.0551 | loss 0.0377 | 5273.1s\n",
      "step  3600 | train_mean 4.0562 | greedy_mean 4.0169 | baseline 4.0531 | loss -0.0511 | 5513.7s\n",
      "step  3800 | train_mean 4.1080 | greedy_mean 4.0615 | baseline 4.0746 | loss -0.0232 | 5769.5s\n",
      "step  4000 | train_mean 4.0315 | greedy_mean 4.0572 | baseline 4.0434 | loss -0.0128 | 6014.6s\n",
      "coords: [[0.9755991  0.5138286 ]\n",
      " [0.9699024  0.36079663]\n",
      " [0.6687073  0.23106426]\n",
      " [0.73278266 0.8510801 ]\n",
      " [0.5727143  0.4401905 ]\n",
      " [0.04589146 0.49850315]\n",
      " [0.6806613  0.77908635]\n",
      " [0.11641258 0.3713045 ]\n",
      " [0.02924722 0.11504579]\n",
      " [0.24943858 0.5467358 ]\n",
      " [0.2912051  0.00533378]\n",
      " [0.6990422  0.7296103 ]\n",
      " [0.8358     0.73186666]\n",
      " [0.40377223 0.36178505]\n",
      " [0.18856591 0.14190519]\n",
      " [0.73082775 0.81722295]\n",
      " [0.31776708 0.26787376]\n",
      " [0.2696858  0.23543954]\n",
      " [0.87507325 0.6638038 ]\n",
      " [0.37006962 0.94750875]]\n",
      "tour  : [10 14  8 17 16 13  7  5  9 19  6  3 15 11 12 18  0  1  2  4]\n",
      "len   : 4.041751384735107\n"
     ]
    }
   ],
   "source": [
    "# drl_tsp_reinforce_pytorch.py\n",
    "# Minimal DRL TSP (Euclidean) with Transformer encoder + autoregressive selection (REINFORCE)\n",
    "# pip install torch  (PyTorch only)\n",
    "\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def tour_length(coords: torch.Tensor, tour: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    coords: [B, N, 2]\n",
    "    tour:   [B, N]  (permutation indices)\n",
    "    returns: [B] length\n",
    "    \"\"\"\n",
    "    B, N, _ = coords.shape\n",
    "    idx = tour.unsqueeze(-1).expand(B, N, 2)\n",
    "    ordered = coords.gather(1, idx)  # [B, N, 2]\n",
    "    # close the tour\n",
    "    rolled = torch.roll(ordered, shifts=-1, dims=1)\n",
    "    seg = (ordered - rolled).pow(2).sum(-1).sqrt()  # [B, N]\n",
    "    return seg.sum(-1)  # [B]\n",
    "\n",
    "def masked_softmax(logits: torch.Tensor, mask: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    logits: [..., N]\n",
    "    mask:   [..., N]  True=available, False=masked\n",
    "    \"\"\"\n",
    "    # make masked positions very negative\n",
    "    masked_logits = logits.masked_fill(~mask, -1e9)\n",
    "    return F.softmax(masked_logits, dim=dim)\n",
    "\n",
    "def sample_from_probs(probs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    probs: [B, N]\n",
    "    returns: [B] sampled indices\n",
    "    \"\"\"\n",
    "    # Multinomial expects probs sum to 1\n",
    "    return torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "\n",
    "def greedy_from_probs(probs: torch.Tensor) -> torch.Tensor:\n",
    "    return probs.argmax(dim=-1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model: Transformer encoder + pointer-like decoder\n",
    "# -----------------------------\n",
    "class TSPPolicy(nn.Module):\n",
    "    def __init__(self, d_model=128, n_heads=8, n_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Linear(2, d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=4*d_model,\n",
    "            dropout=dropout, batch_first=True, activation=\"relu\"\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        # Decoder state: use (last_selected_node_embedding) as query\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # initial \"start token\" (learnable) as first query\n",
    "        self.start = nn.Parameter(torch.randn(d_model))\n",
    "\n",
    "        # temperature (optional)\n",
    "        self.log_temp = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, coords: torch.Tensor, sample: bool = True):\n",
    "        \"\"\"\n",
    "        coords: [B, N, 2]\n",
    "        returns:\n",
    "          tour: [B, N]\n",
    "          logp: [B] sum log-prob of sampled actions\n",
    "        \"\"\"\n",
    "        B, N, _ = coords.shape\n",
    "        x = self.embed(coords)                    # [B, N, D]\n",
    "        h = self.encoder(x)                       # [B, N, D]\n",
    "\n",
    "        # Precompute keys\n",
    "        K = self.k_proj(h)                        # [B, N, D]\n",
    "        # availability mask (unvisited)\n",
    "        avail = torch.ones(B, N, dtype=torch.bool, device=coords.device)\n",
    "\n",
    "        tour = []\n",
    "        logps = []\n",
    "\n",
    "        # initial query\n",
    "        q = self.start.unsqueeze(0).expand(B, self.d_model)  # [B, D]\n",
    "\n",
    "        for t in range(N):\n",
    "            Q = self.q_proj(q).unsqueeze(1)                  # [B, 1, D]\n",
    "            # attention logits: Q*K^T\n",
    "            logits = torch.bmm(Q, K.transpose(1, 2)).squeeze(1)  # [B, N]\n",
    "\n",
    "            # temperature\n",
    "            temp = torch.exp(self.log_temp).clamp(0.1, 10.0)\n",
    "            logits = logits / temp\n",
    "\n",
    "            probs = masked_softmax(logits, avail, dim=-1)    # [B, N]\n",
    "\n",
    "            if sample:\n",
    "                a = sample_from_probs(probs)                 # [B]\n",
    "                lp = torch.log(probs.gather(1, a.unsqueeze(1)).squeeze(1) + 1e-12)\n",
    "            else:\n",
    "                a = greedy_from_probs(probs)\n",
    "                lp = torch.log(probs.gather(1, a.unsqueeze(1)).squeeze(1) + 1e-12)\n",
    "\n",
    "            tour.append(a)\n",
    "            logps.append(lp)\n",
    "\n",
    "            # update mask\n",
    "            avail.scatter_(1, a.unsqueeze(1), False)\n",
    "\n",
    "            # update query as the chosen node embedding (from h)\n",
    "            q = h.gather(1, a.view(B, 1, 1).expand(B, 1, self.d_model)).squeeze(1)\n",
    "\n",
    "        tour = torch.stack(tour, dim=1)         # [B, N]\n",
    "        logp = torch.stack(logps, dim=1).sum(1) # [B]\n",
    "        return tour, logp\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training (REINFORCE)\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    n_nodes: int = 20\n",
    "    batch_size: int = 256\n",
    "    steps: int = 4000\n",
    "    lr: float = 1e-4\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print_every: int = 200\n",
    "    seed: int = 42\n",
    "    baseline_beta: float = 0.9  # moving average\n",
    "\n",
    "def train():\n",
    "    cfg = TrainConfig()\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    policy = TSPPolicy(d_model=128, n_heads=8, n_layers=3, dropout=0.1).to(cfg.device)\n",
    "    opt = torch.optim.Adam(policy.parameters(), lr=cfg.lr)\n",
    "\n",
    "    baseline = None\n",
    "    t0 = time.time()\n",
    "\n",
    "    for step in range(1, cfg.steps + 1):\n",
    "        # random Euclidean TSP instances\n",
    "        coords = torch.rand(cfg.batch_size, cfg.n_nodes, 2, device=cfg.device)\n",
    "\n",
    "        tour, logp = policy(coords, sample=True)\n",
    "        L = tour_length(coords, tour)  # [B]  minimize this\n",
    "\n",
    "        # moving average baseline\n",
    "        with torch.no_grad():\n",
    "            batch_mean = L.mean()\n",
    "            if baseline is None:\n",
    "                baseline = batch_mean\n",
    "            else:\n",
    "                baseline = cfg.baseline_beta * baseline + (1 - cfg.baseline_beta) * batch_mean\n",
    "\n",
    "        adv = (L - baseline).detach()  # [B]\n",
    "        loss = (adv * logp).mean()     # REINFORCE\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        if step % cfg.print_every == 0:\n",
    "            # evaluate greedy\n",
    "            with torch.no_grad():\n",
    "                coords_val = torch.rand(1024, cfg.n_nodes, 2, device=cfg.device)\n",
    "                tour_g, _ = policy(coords_val, sample=False)\n",
    "                Lg = tour_length(coords_val, tour_g).mean().item()\n",
    "\n",
    "            elapsed = time.time() - t0\n",
    "            print(f\"step {step:5d} | train_mean {L.mean().item():.4f} | greedy_mean {Lg:.4f} | baseline {baseline.item():.4f} | loss {loss.item():.4f} | {elapsed:.1f}s\")\n",
    "\n",
    "    return policy, cfg\n",
    "\n",
    "def demo(policy: TSPPolicy, n_nodes=20, device=\"cpu\"):\n",
    "    policy.eval()\n",
    "    coords = torch.rand(1, n_nodes, 2, device=device)\n",
    "    with torch.no_grad():\n",
    "        tour, _ = policy(coords, sample=False)\n",
    "        L = tour_length(coords, tour).item()\n",
    "    print(\"coords:\", coords.squeeze(0).cpu().numpy())\n",
    "    print(\"tour  :\", tour.squeeze(0).cpu().numpy())\n",
    "    print(\"len   :\", L)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    policy, cfg = train()\n",
    "    demo(policy, n_nodes=cfg.n_nodes, device=cfg.device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
