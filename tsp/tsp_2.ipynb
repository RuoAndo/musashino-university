{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70eb7436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.10.0+cpu cuda_available: False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [CPUBoolType [32, 10]] is at version 10; expected version 9 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m policy\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtorch:\u001b[39m\u001b[33m\"\u001b[39m, torch.__version__, \u001b[33m\"\u001b[39m\u001b[33mcuda_available:\u001b[39m\u001b[33m\"\u001b[39m, torch.cuda.is_available())\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m policy = \u001b[43mtrain_minimal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m coords = sample_batch(\u001b[32m1\u001b[39m, \u001b[32m10\u001b[39m)\n\u001b[32m    113\u001b[39m tour, _ = policy(coords, greedy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mtrain_minimal\u001b[39m\u001b[34m(n_nodes, batch_size, steps, lr, print_every)\u001b[39m\n\u001b[32m     94\u001b[39m loss = -(adv.detach() * logp).mean()\n\u001b[32m     96\u001b[39m opt.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m torch.nn.utils.clip_grad_norm_(policy.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     99\u001b[39m opt.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\flare\\miniforge3\\envs\\pymc_env\\Lib\\site-packages\\torch\\_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\flare\\miniforge3\\envs\\pymc_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\flare\\miniforge3\\envs\\pymc_env\\Lib\\site-packages\\torch\\autograd\\graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: one of the variables needed for gradient computation has been modified by an inplace operation: [CPUBoolType [32, 10]] is at version 10; expected version 9 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# Notebook最小：強制CPUでクラッシュ回避版\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # ★torch importより前！\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"  # ★DLL衝突対策（必要な場合のみ）\n",
    "\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "DEVICE = \"cpu\"  # ★強制CPU（ここが重要）\n",
    "\n",
    "def tsp_tour_length(coords, tour):\n",
    "    B, N, _ = coords.shape\n",
    "    idx = tour.unsqueeze(-1).expand(B, N, 2)\n",
    "    ordered = coords.gather(1, idx)\n",
    "    rolled = torch.roll(ordered, shifts=-1, dims=1)\n",
    "    seg = (ordered - rolled).norm(p=2, dim=-1)\n",
    "    return seg.sum(dim=1)\n",
    "\n",
    "def sample_batch(batch_size, n_nodes):\n",
    "    return torch.rand(batch_size, n_nodes, 2, device=DEVICE)\n",
    "\n",
    "class PointerPolicy(nn.Module):\n",
    "    def __init__(self, embed_dim=64):  # ★軽く\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(2, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.scale = 1.0 / math.sqrt(embed_dim)\n",
    "\n",
    "    def forward(self, coords, greedy=False):\n",
    "        B, N, _ = coords.shape\n",
    "        E = self.embed(coords)\n",
    "        K = self.k_proj(E)\n",
    "\n",
    "        cur = torch.zeros(B, dtype=torch.long, device=coords.device)\n",
    "        visited = torch.zeros(B, N, dtype=torch.bool, device=coords.device)\n",
    "        visited[:, 0] = True\n",
    "\n",
    "        tour = []\n",
    "        logps = []\n",
    "\n",
    "        for t in range(N):\n",
    "            tour.append(cur)\n",
    "            if t == N - 1:\n",
    "                break\n",
    "\n",
    "            q = self.q_proj(E[torch.arange(B, device=coords.device), cur])\n",
    "            scores = (K * q.unsqueeze(1)).sum(dim=-1) * self.scale\n",
    "            scores = scores.masked_fill(visited, -1e9)\n",
    "            probs = F.softmax(scores, dim=-1)\n",
    "\n",
    "            if greedy:\n",
    "                nxt = probs.argmax(dim=-1)\n",
    "                logp = torch.log(probs.gather(1, nxt.unsqueeze(1)).squeeze(1) + 1e-12)\n",
    "            else:\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                nxt = dist.sample()\n",
    "                logp = dist.log_prob(nxt)\n",
    "\n",
    "            logps.append(logp)\n",
    "            visited[torch.arange(B, device=coords.device), nxt] = True\n",
    "            cur = nxt\n",
    "\n",
    "        tour = torch.stack(tour, dim=1)\n",
    "        logp = torch.stack(logps, dim=1).sum(dim=1)\n",
    "        return tour, logp\n",
    "\n",
    "def train_minimal(n_nodes=10, batch_size=32, steps=50, lr=1e-3, print_every=10):\n",
    "    policy = PointerPolicy(embed_dim=64).to(DEVICE)\n",
    "    opt = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    baseline = None\n",
    "    beta = 0.9\n",
    "\n",
    "    for step in range(1, steps + 1):\n",
    "        coords = sample_batch(batch_size, n_nodes)\n",
    "\n",
    "        tour, logp = policy(coords, greedy=False)\n",
    "        length = tsp_tour_length(coords, tour)\n",
    "        reward = -length\n",
    "\n",
    "        with torch.no_grad():\n",
    "            r_mean = reward.mean()\n",
    "            baseline = r_mean if baseline is None else (beta * baseline + (1 - beta) * r_mean)\n",
    "            adv = reward - baseline\n",
    "\n",
    "        loss = -(adv.detach() * logp).mean()\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            coords_eval = sample_batch(128, n_nodes)\n",
    "            tour_g, _ = policy(coords_eval, greedy=True)\n",
    "            len_g = tsp_tour_length(coords_eval, tour_g).mean().item()\n",
    "            print(f\"step {step:4d} | loss {loss.item():+.4f} | greedy mean length {len_g:.4f}\")\n",
    "\n",
    "    return policy\n",
    "\n",
    "print(\"torch:\", torch.__version__, \"cuda_available:\", torch.cuda.is_available())\n",
    "policy = train_minimal()\n",
    "\n",
    "coords = sample_batch(1, 10)\n",
    "tour, _ = policy(coords, greedy=True)\n",
    "print(\"tour:\", tour.squeeze(0).tolist())\n",
    "print(\"length:\", tsp_tour_length(coords, tour).item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
